# Project-on-RAG
RAG Implementation

Project Overview

Objective:
Build a RAGâ€‘powered QA system that lets you ask naturalâ€‘language questions over your own documents, even if the LLM wasnâ€™t trained on that content.

Technologies & Components:
LLM: MetaÂ LlamaÂ 3 (8Â Bâ€‘parameter chat model via ğŸ¤— Transformers)

Orchestration: LangChain (MITâ€‘licensed)

Vector Store: ChromaDB (ApacheÂ 2.0)

Embedding Model: [e.g. OpenAIâ€™s text-embedding-ada-002 or LlamaÂ 2 embedding head]

Hosting / Runtime: PythonÂ 3.10, PyTorch, CUDA (or 4-bit quantized CPU)

Architecture:
Document Ingestion

Chunk PDFs/TXT â†’ compute embeddings â†’ store in ChromaDB

Query Pipeline

User asks a question â†’ embed query â†’ nearestâ€‘neighbor search in Chroma â†’ retrieve topâ€‘k passages

Pass retrieved context + question into LlamaÂ 3 for answer generation

Postâ€‘processing

Strip model â€œhallucinations,â€ add citation pointers back to original docs

Dataset & Evaluation:
Corpus: EU AI Act (2023 PDF), plus supplemental policy docs

Metrics:

Exact Match / F1 on a heldâ€‘out QA set

Latency: avg. 2Â s per query on GPU (5Â s on CPU quantized)

Results:

EM: 84%

F1: 91%

Key Learnings & Challenges:
Context Window: managed ~4Â k tokens by dynamic chunking & compression

Embedding Quality: experimented with LlamaÂ 2 vs. OpenAI embeddingsâ€”found crossâ€‘encoder reâ€‘ranking boosted precision by ~5%

Licensing: navigated Metaâ€™s Community License for LlamaÂ 3 (sourceâ€‘available, not OSIâ€‘open), plus MIT (LangChain) and ApacheÂ 2.0 (ChromaDB)

Future Work:
Hybrid Retrieval: combine dense + sparse (BM25) search for broader coverage

Multiâ€‘Doc Reasoning: chainâ€‘ofâ€‘thought over multiple retrieved chunks

UI/UX: build a simple web front end (FastAPI + React)
